---
title: "Dmel Social Environment on Behaviour"
author: "Erin Macartney"
date: "`r Sys.Date()`"
output:
  rmdformats::robobook:
      code_folding: show
      toc_depth: 3
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message = F}
library(tidyverse)
library(lme4)
library(lmerTest)
library(here) # write out the path
library(kableExtra)
library(sjlabelled)
library(ggpubr)
library(rstatix)
library(car)
library(RColorBrewer)
library(readxl)

```

## Data parsing
Loading session data with treatments and sex etc

```{r 1_data_load}
#Loading in session data
Session_data <- read_excel(here("Data/session_data/Session_data_extended.xlsx"))
```

#### Locomotion

First let's test-load one data file to see how to trim it into relevant bits. Definition in this files is so that first 4 rows and first 6 columns are redundant.

The first portion loads and parses the raw data file lines.

```{r 1_data_load}
data_path <- here("Data/locomotion/")
data_files <- list.files(here("Data/locomotion/"), recursive = T)

#This loads all 24 files but we will only parse one data file for now
data_files[1:24]
```

First - we process a sample CSV file, which is unstructured and contains a lot of spurious lines and data (e.g., control variables and comments generated by the unit).

```{r 2_parse_lines}
# read in one specific file for Mac users 
#dat_temp <- readLines(paste(data_path, data_files[1], sep = ''))

# # read in one specific file for PC users 
dat_temp <- readLines(paste(data_path, data_files[1], sep = '/'))

# glimpse(dat_temp)

```

The loaded data is just a vector of strings, each being a line from the original CSV file. In the next chunk we skip the first 4 lines (fixed number, lines containing technical parameters of the units), and load 5 lines (which excludes the header line - so in fact 6 lines), skipping the last technical line. At the end we clean the file (removing empty columns `2:6` and wells `c(F4, F5, F6, F7, F8)` - they are always empty in our system). Adjust as needed. The resulting file contains each assayed well as separate column, and for each there are 5 times intervals of locomotion monitoring.

```{r 3_read_metadata}
dat_temp_df <- read.csv(text = dat_temp, header = T, sep = ',',
                        quote = '\"', dec = '.', skip = 4, nrows = length(dat_temp) - 4 - 2,
                        stringsAsFactors = F)

dat_temp_df <- dat_temp_df[, -(2:6)] # remove spurious columns
dat_temp_df <- dat_temp_df %>% select(!(F3:F8)) #remove wells that do not contain data
dat_temp_df

glimpse(dat_temp_df)
```

Here we extract and append the run (subject) ID.

```{r 4_parse_runID}
head_temp <- readLines(paste(data_path, data_files[1], sep = '/'), n = 4)

id_index <- grep('Subject Identification', head_temp)

run_id <- gsub('.*Subject Identification\\\",\"([A-Z]{1}[0-9]{3})\\\"', '\\1', head_temp[id_index])
dat_temp_df$Datafile_ID <- run_id

# assay_date <- str_sub(run_id, 11, 16)
# dat_temp_df$date <- as.Date(assay_date, format = "%y%m%d")
glimpse(dat_temp_df)
```

Change to long format.

```{r 5_turn_to_long}
dat_temp_df2 <-
  dat_temp_df %>%
  pivot_longer(names_to = 'well_id', values_to = 'arena_distance', cols = matches('[A-H][1-9]')) %>%
  rename(time = TIME, temperature = TEMPERATURE, round = ROUND,
         variable = VARIABLE)

# head(dat_temp_df2)
glimpse(dat_temp_df2)
```

We have to add the `Individuals_ID` variable to be able to link locomotion data to sex data. It will serve to link Datafile_ID with respective Individuals_ID - and through it with appropriate well number. The run register contains several variables used to group individuals into several blocks that may share some of the systematic variation.


```{r 7_merge_metadata}
# run_register <- read_delim(here('Data', 'run_data', 'run_register_pilot2.csv'), delim = ';')
# glimpse(run_register)

#Session_data
Session_data$Individual_ID <- paste(Session_data$Assay_ID,Session_data$Position, sep = "_")

#data_temp
dat_temp_df2$Individual_ID <- paste(dat_temp_df2$Datafile_ID, dat_temp_df2$well_id, sep = "_")


dat_temp_df3 <- dat_temp_df2 %>%
  left_join(Session_data, by = "Individual_ID")
glimpse(dat_temp_df3)

```


```{r new_data_compiler}
data_compiler <- function(filename, data_path) {
  
  ## ARG filename vector or list of file names to process (only files, no full paths)
  ## ARG data_path text string with the path to access all files
  ## ARG run_register database of all runs with datafile IDs and sexing IDs
  
  ## ARGS to develop: passing custom RE, custom wells to skip, custom columns to skip
  
  dat_temp <- readLines(paste(data_path, filename, sep = '/'))
  
  # file parsing
  dat_temp_df <- read.csv(text = dat_temp, header = T, sep = ',',
                          quote = '\"', dec = '.', skip = 4, nrows = length(dat_temp) - 4 - 2,
                          stringsAsFactors = F)
  
  ## these line are design specific - for now the function has close form on those
  ## possible to implement as additional argument
  
  dat_temp_df <- dat_temp_df[, -(2:6)]
  dat_temp_df <- dat_temp_df %>% select(!(F3:F8))
  
  # extract headers
  head_temp <- readLines(paste(data_path, filename, sep = '/'), n = 4)
  
  # using RE - this method is more flexible as the structure and location of ID can change
  id_index <- grep('Subject Identification', head_temp)
  run_id <- gsub('.*Subject Identification\\\",\"([A-Z]{1}[0-9]{3})\\\"', '\\1', head_temp[id_index])
  dat_temp_df$Datafile_ID <- run_id
  
  # assay_date <- str_sub(run_id, 11, 16)
  # dat_temp_df$date <- assay_date
  
  dat_temp_df <-
  dat_temp_df %>%
  pivot_longer(names_to = 'well_id', values_to = 'arena_distance', cols = matches('[A-H][1-9]')) %>%
  rename(time = TIME, temperature = TEMPERATURE, round = ROUND,
         variable = VARIABLE)
  
  dat_temp_df$Individual_ID <- paste(dat_temp_df$Datafile_ID, dat_temp_df$well_id, sep = "_")
  
  # dat_temp_df <- dat_temp_df %>%
  #   left_join(select(run_register, Datafile_ID, Individuals_ID, Exp_block, Batch_ID))
  # 
  # dat_temp_df <- dat_temp_df %>%
  #   mutate(Individuals_ID_well = paste0(Individuals_ID, '_', Well_ID))
  # 
  # dat_temp_df$Filename <- filename
  
  dat_temp_df <- dat_temp_df %>% select(Individual_ID, Datafile_ID,
                                      temperature, round, arena_distance)

  return(dat_temp_df)
}
```


```{r 9_parse_data, message = F}
# Not run: test
# data_compiler(data_files[1], data_path, run_register)

data_locomotion <- map_dfr(data_files, ~ data_compiler(.x, data_path = data_path))

length(unique(data_locomotion$Datafile_ID)) # should be 8 distinct files

glimpse(data_locomotion)
```

```{r merge_alrge_data_with_session_data}
data_locomotion <- data_locomotion %>%
  left_join(Session_data, by = "Individual_ID")
glimpse(data_locomotion)
```



#### Ymaze

First we load the y-maze data and tidy it up to separate summary data from zone changes data.

```{r 17_load_data}
## parse datafiles into a nested tibble
data_path <- here('Data/ymaze/')
data_files <- list.files(here('Data/ymaze/'), recursive = T)
data_files[1:72]
```

The below parser identifies lines in the dataset that directly relate to zone-switching data and extracts them, or (when `summary = TRUE`) it extracts the arena distances summaries from the bottom section of the file.

*[EDIT] apparently read_delim no longer works with just vectors, now needs `I()`*
```{r 18_create_parser}
# helper functions extracting the data rows and the header rows
data_parser <- function(pattern, path, filename, summary = FALSE) {
  dat_temp <- readLines(paste(path, filename, sep = '/'))
  zone_change_index <- grep(pattern, dat_temp)
  
  if(summary == FALSE) {
    return(read_delim(file = I(dat_temp[zone_change_index]), col_names = F, delim = ',',
                      quote = '\"' # skip = 4, nrows = length(dat_temp) - 4 - 1)
    ))
  } else {
    return(read_delim(file = I(dat_temp[-zone_change_index]), col_names = T, delim = ',',
                      quote = '\"', skip = 6, n_max = 3))
  }
}

# test the parser
# here we simply check that the selected pattern to be looked for (`Arena`) is indeed present in the file.
pattern1 <- "Arena" # defines what should contain each line that we look for
test1 <- readLines(paste(data_path, data_files[1], sep = '/'))
id_test <- grep(pattern1, test1) # use (simplified) RE to select lines
```

*[EDIT] apparently read_delim no longer works with just vectors, now needs `I()`*
```{r 19_test_data}
# this extracts (messy) run and temporary data from each file
test_dat <- read_delim(file = I(test1[-id_test]), col_names = T, delim = ',',
                      quote = '\"', skip = 6, n_max = 3)

# helper function that parses the (messy) run data into a readable format)
head_parser <- function(path, filename) {
  dat_temp <- readLines(paste(path, filename, sep = '/'), n = 4)
  return(read_delim(file = I(dat_temp),
                    delim = ',',
                    col_names = F))
}
```


```{r 20_parse_data_into_tibbles}
# create tibble with raw data (nested: data and header nested under file names)
dat_raw <- tibble(data_files = data_files) %>%
  mutate(data = map(data_files, 
                    ~ data_parser(pattern1,
                                  data_path,
                                  .x, summary = FALSE))) %>%
  mutate(head = map(data_files, ~ head_parser(data_path, .x)))

# this removes the redundant 'Arena' column
dat_raw[[2]] <- dat_raw[[2]] %>%
  map(~ select(.x, !c(X3)))

# rename variables in sub-tibble based on their real content
dat_raw[[2]] <- dat_raw[[2]] %>%
  map(~ rename(.x, time = X1, Info = X2, Arena = X4, Action = X5, Zone_no = X6))

# reformat the head sub-tibble
dat_raw[[3]] <- dat_raw[[3]] %>%
  map(~ pivot_wider(.x, names_from = X3, values_from = X4)) %>%
  map(~ rename(.x, Datafile_ID = `Subject Identification`)) %>%
  map(~ select(.x, Apparatus, Datafile_ID))

# check the modifications
dat_raw[[2]][[1]]
dat_raw[[3]][[1]]
```


```{r 21_process_summaries}
# parse summary data from a file into a separate tibble - this is not used for now
# processed only for consistency
dat_raw_summ <- tibble(data_files = data_files) %>%
  mutate(data = map(data_files, ~ data_parser(pattern1,
                                data_path,
                                .x,
                                summary = T)))
dat_raw_summ[[2]] <- dat_raw_summ[[2]] %>%
  map(~ select(.x, !c(...1, ...2, ...3))) %>%
  map(~ rename(.x, Summary_stat = ...4))
dat_raw_summ[[2]][[1]]
```

```{r 13_unnest}

dat_un <- dat_raw %>%
  unnest(head) %>%
  unnest(data)

dat_un <- dat_un %>%
  select(data_files, Datafile_ID, time, Arena, Action, Zone_no) %>%
  rename(Data_file = data_files)

```

```{r 24_turn_towide}
dat_un <- dat_un %>%
  mutate(Row_ID = row_number()) %>%
  pivot_wider(names_from = Action, values_from = Zone_no) %>%
  rename(Exit_zone = Exit_Zone, Enter_zone = Enter_Zone)
dat_un
```

```{r 25_create_time_bins}
dat_an <- dat_un
dat_an <- dat_an %>%
  mutate(Bin = ifelse(time > 600 & time < 1200, 1,
                      ifelse(time > 1200 & time < 1800, 2,
                             ifelse(time > 1800 & time < 2500, 3, NA))))

dat_an <- dat_an %>%
  arrange(Data_file, Datafile_ID, Arena, time, Exit_zone)
dat_an
```

```{r 26_error_check_create_exit_entry_instances}
dat_an <- dat_an %>%
  mutate(Zone = ifelse(Enter_zone == lead(Exit_zone), Enter_zone, 666)) %>%
  mutate(t_enter = ifelse(Enter_zone >= 1, time, 666)) %>%
  mutate(t_exit = ifelse(Exit_zone >= 1, time, 666))

dat_an %>% filter(Zone == 666)
dat_an %>% filter(t_enter == 666)
dat_an %>% filter(t_exit == 666)

# ALL GOOD! Note - this step is very important and serves to test if data points were sorted correctly
# Single detected mistakes are likely final entries that failed to exit before assay end


# Let's filter out the non-conforming cases and re-confirm the rest is ordered correctly
# This stage is critical: dplyr filter() function is terribly unintuitive - with the logical
# condition "NOT" (!) it also drops NA values - thus we have to ensure with replace_na() they are kept
# to preserve the ordering of enter-exit events

dat_an <- dat_an %>% filter((Zone != 666) %>% replace_na(TRUE))
dat_an <- dat_an %>%
  arrange(Data_file, Datafile_ID, Arena, time, Exit_zone)
dat_an <- dat_an %>%
  mutate(Zone = ifelse(Enter_zone == lead(Exit_zone), Enter_zone, 666)) %>%
  mutate(t_enter = ifelse(Enter_zone >= 1, time, 666)) %>%
  mutate(t_exit = ifelse(Exit_zone >= 1, time, 666))
dat_an %>% filter(Zone == 666)
# repeat above 4 steps until last line of code returns empty tibble
```

```{r 27_calculate_zone_times}
dat_an <- dat_an %>%
  # filter(is.na(Zone)) %>% # this filter is just for testing purposes (it removes the 'Exit_zone' portion of data)
  select(Data_file, Datafile_ID, time, Arena, Row_ID, Bin, Zone, t_enter, t_exit)
dat_an <- dat_an %>%
  mutate(t_exit = lead(t_exit))
dat_an <- na.omit(dat_an)
dat_an <- dat_an %>%
  mutate(t_zone = t_exit - t_enter)
dat_an
```

```{r 28_remove_central_zone}
dat_an2 <- dat_an %>% filter(Zone != 4)
dat_an2
```

Movement analysis [in progress, not running yet --> see issues]

```{r 29_add_meta_make_trigrams}
well_arena <- read_delim(here('Data', 'run_data', 'well_arena_corresp.csv'), delim = ';')

dat_an2 <- dat_an2 %>%
  left_join(select(run_register, Datafile_ID, Individuals_ID, Plate, Unit_ID), by = "Datafile_ID") %>%
  mutate(Ymaze_arena = paste0(Plate, "_", Arena)) %>%
  # mutate(fly_id = paste(gsub('[A-Za-z0-9]+\\/([A-Za-z0-9_-]+)\\.csv', '\\1', data_files), arena, sep = "_"))
  # mutate(fly_id = paste0(Datafile_ID, "_", Ymaze_arena)) %>% # not really necessary
  left_join(select(well_arena, Ymaze_arena, Plate48well), by = "Ymaze_arena") %>%
  mutate(Individuals_ID_well = paste0(Individuals_ID, "_", Plate48well)) %>%
  select(Datafile_ID, Individuals_ID, Individuals_ID_well, Ymaze_arena, time, Bin, Zone, t_zone)

# split datafile into individual flies
dat_an2 <- dat_an2 %>% split(., .[, "Individuals_ID_well"])

dat_an2 <- dat_an2 %>%
  map(~ mutate(.x, Lag_zone = lag(Zone))) %>%
  map(~ mutate(.x, Turn = case_when(Lag_zone==1 & Zone==2 ~ 'L',
                                  Lag_zone==1 & Zone==3 ~ 'R',
                                  Lag_zone==2 & Zone==1 ~ 'R',
                                  Lag_zone==2 & Zone==3 ~ 'L',
                                  Lag_zone==3 & Zone==1 ~ 'L',
                                  Lag_zone==3 & Zone==2 ~ 'R',
                                  # lag_zone==zone ~ 'X', # this enables additional decision type = stay in the given zone
                                  TRUE ~ NA_character_ ))) %>%
  map(~ select(.x, Datafile_ID, Individuals_ID, Individuals_ID_well, Ymaze_arena, time, Bin, Zone, t_zone, Turn))

dat_an3 <- bind_rows(dat_an2)
dat_an3 <- dat_an3 %>%
  arrange(Datafile_ID, Individuals_ID_well, Bin)

dat_an4 <- na.omit(dat_an3)
dat_an4
```

#### Startle response

```{r 38_load_data}
#specify directory of zantiks habituation files
data_path <- here("Data/habituation/") 

#create list of files from directory
file_list <- list.files(data_path)

#create header from first file
df <-
  paste(data_path, file_list[1],sep = '/') %>%
  read_csv(skip=4,col_names = TRUE, guess_max = 100) %>%
  head(0)

#create new list without demographic info
new_list<- c()

for (i in file_list){
  new_list[[i]] <-
    read_csv(paste(data_path, i, sep = '/'),
             skip=4, col_names = TRUE, guess_max = 100) %>%
    head(-1)
}

#append all files to df
for (i in new_list){
  df<-add_row(df,i)
}
```

```{r 39_select_variables}
df <- df %>% select(!c(RUNTIME, UNIT, TIMESLOT, TEMPERATURE))
#convert variables to factors for anova
df<-as_factor(df,BLOCK)
df<-as_factor(df,TYPE)

df <- df %>% rename(Datafile_ID = PLATE_ID, time_bin = TIME_BIN,
                    Block = BLOCK, Trial = TRIAL, Type = TYPE,
                    pre_post_counter = PRE_POST_COUNTER,
                    startle_number = STARTLE_NUMBER)

df <- df %>% select(-c(F6:F8), -c(F6MSD:F8MSD))
```

```{r 41_MSD_process_etal}
dfile_dist <- df %>% 
  select(!ends_with("MSD")) %>%
  gather(key = "Well", value = "distance", -Datafile_ID,
         -time_bin, -Block, -Trial, -Type, -pre_post_counter,
         -startle_number) %>%
  convert_as_factor(Well)

# create file with well factor and MSD activity dv only
dfile_act<- df %>%
  select(ends_with("MSD")) %>%
  gather(key = "Well", value = "activity") %>%
  convert_as_factor(Well)

# remove duplicate well variable before adding activity data
dfile_act <- select(dfile_act, -'Well')

# add activity column to rest of data
df <- add_column(dfile_dist, dfile_act)

# remove acclimation data
no_acclimation <- df %>%
  filter(Type != "ACCLIMATION")

# create data file with only startles from first repeat (Block == 1)
startles_only <- filter(no_acclimation, Type == "STARTLE", Block == 1)
# pop_startles_only<-filter(pop_data, Type == "STARTLE", Block == 1)
```

Merging with sex (and other data) [in progress, not running yet --> see issue]

```{r 41_merge_with_sex_data}
startles_only <- startles_only %>%
    left_join(select(run_register, Datafile_ID, Individuals_ID, Exp_block, Batch_ID))

startles_only <- startles_only %>%
  mutate(Individuals_ID_well = paste0(Individuals_ID, "_", Well))

startles_only <- startles_only %>%
  left_join(sex, by = "Individuals_ID_well")
```