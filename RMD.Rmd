---
title: "Dmel Social Environment on Behaviour"
author: "Erin Macartney"
date: "`r Sys.Date()`"
output:
  rmdformats::robobook:
      code_folding: show
      toc_depth: 3
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message = F}
library(tidyverse)
library(lme4)
library(lmerTest)
library(here) # write out the path
library(kableExtra)
library(sjlabelled)
library(ggpubr)
library(rstatix)
library(car)
library(RColorBrewer)
library(readxl)

```

## Data parsing
Loading session data with treatments and sex etc

```{r 1_data_load}
#Loading in session data
Session_data <- read_excel(here("Data/session_data/Session_data_extended.xlsx"))
```

### Sample dataframes

#### Locomotion

First let's test-load one data file to see how to trim it into relevant bits. Definition in this files is so that first 4 rows and first 6 columns are redundant.

The first portion loads and parses the raw data file lines.

```{r 1_data_load}
data_path <- here("Data/locomotion/")
data_files <- list.files(here("Data/locomotion/"), recursive = T)

#This loads all 24 files but we will only parse one data file for now
data_files[1:24]
```

First - we process a sample CSV file, which is unstructured and contains a lot of spurious lines and data (e.g., control variables and comments generated by the unit).

```{r 2_parse_lines}
# read in one specific file for Mac users 
#dat_temp <- readLines(paste(data_path, data_files[1], sep = ''))

# # read in one specific file for PC users 
dat_temp <- readLines(paste(data_path, data_files[1], sep = '/'))

# glimpse(dat_temp)

```

The loaded data is just a vector of strings, each being a line from the original CSV file. In the next chunk we skip the first 4 lines (fixed number, lines containing technical parameters of the units), and load 5 lines (which excludes the header line - so in fact 6 lines), skipping the last technical line. At the end we clean the file (removing empty columns `2:6` and wells `c(F4, F5, F6, F7, F8)` - they are always empty in our system). Adjust as needed. The resulting file contains each assayed well as separate column, and for each there are 5 times intervals of locomotion monitoring.

```{r 3_read_metadata}
dat_temp_df <- read.csv(text = dat_temp, header = T, sep = ',',
                        quote = '\"', dec = '.', skip = 4, nrows = length(dat_temp) - 4 - 2,
                        stringsAsFactors = F)

dat_temp_df <- dat_temp_df[, -(2:6)] # remove spurious columns
dat_temp_df <- dat_temp_df %>% select(!(F3:F8)) #remove wells that do not contain data
dat_temp_df

glimpse(dat_temp_df)
```

Here we extract and append the run (subject) ID.

```{r 4_parse_runID}
head_temp <- readLines(paste(data_path, data_files[1], sep = '/'), n = 4)

id_index <- grep('Subject Identification', head_temp)

run_id <- gsub('.*Subject Identification\\\",\"([A-Z]{1}[0-9]{3})\\\"', '\\1', head_temp[id_index])
dat_temp_df$Datafile_ID <- run_id

# assay_date <- str_sub(run_id, 11, 16)
# dat_temp_df$date <- as.Date(assay_date, format = "%y%m%d")
glimpse(dat_temp_df)
```

Change to long format. .

```{r 5_turn_to_long}
dat_temp_df2 <-
  dat_temp_df %>%
  pivot_longer(names_to = 'well_id', values_to = 'arena_distance', cols = matches('[A-H][1-9]')) %>%
  rename(time = TIME, temperature = TEMPERATURE, round = ROUND,
         variable = VARIABLE)

# head(dat_temp_df2)
# glimpse(dat_temp_df2)
```

We have to add the `Individuals_ID` variable to be able to link locomotion data to sex data. It will serve to link Datafile_ID with respective Individuals_ID - and through it with appropriate well number. The run register contains several variables used to group individuals into several blocks that may share some of the systematic variation.


```{r 7_merge_metadata}
# run_register <- read_delim(here('Data', 'run_data', 'run_register_pilot2.csv'), delim = ';')
# glimpse(run_register)

#Session_data
Session_data$Individual_ID <- paste(Session_data$Assay_ID,Session_data$Position, sep = "_")

#data_temp
dat_temp_df2$Individual_ID <- paste(dat_temp_df2$Datafile_ID, dat_temp_df2$well_id, sep = "_")


dat_temp_df3 <- dat_temp_df2 %>%
  left_join(Session_data, by = "Individual_ID")

```

#### Ymaze

First we load the y-maze data and tidy it up to separate summary data from zone changes data.

```{r 17_load_data}
## parse datafiles into a nested tibble
data_path <- here('Data/ymaze/')
data_files <- list.files(here('Data/ymaze/'), recursive = T)
data_files[1:72]
```

The below parser identifies lines in the dataset that directly relate to zone-switching data and extracts them, or (when `summary = TRUE`) it extracts the arena distances summaries from the bottom section of the file.

```{r 18_create_parser}
# helper functions extracting the data rows and the header rows
data_parser <- function(pattern, path, filename, summary = FALSE) {
  dat_temp <- readLines(paste(path, filename, sep = '/'))
  zone_change_index <- grep(pattern, dat_temp)
  
  if(summary == FALSE) {
    return(read_delim(file = dat_temp[zone_change_index], col_names = F, delim = ',',
                      quote = '\"' # skip = 4, nrows = length(dat_temp) - 4 - 1)
    ))
  } else {
    return(read_delim(file = dat_temp[-zone_change_index], col_names = T, delim = ',',
                      quote = '\"', skip = 6, n_max = 3))
  }
}

# test the parser
# here we simply check that the selected pattern to be looked for (`Arena`) is indeed present in the file.
pattern1 <- "Arena" # defines what should contain each line that we look for
test1 <- readLines(paste(data_path, data_files[1], sep = '/'))
id_test <- grep(pattern1, test1) # use (simplified) RE to select lines
```


```{r 19_test_data}
# this extracts (messy) run and temporary data from each file
test_dat <- read_delim(file = test1[-id_test], col_names = T, delim = ',',
                      quote = '\"', skip = 6, n_max = 3)

# helper function that parses the (messy) run data into a readable format)
head_parser <- function(path, filename) {
  dat_temp <- readLines(paste(path, filename, sep = ''), n = 4)
  return(read_delim(file = dat_temp,
                    delim = ',',
                    col_names = F))
}
```


#### Startle response

```{r 38_load_data}
#specify directory of zantiks habituation files
data_path <- here("data", "habituation/") 

#create list of files from directory
file_list <- list.files(data_path)

#create header from first file
df <-
  paste0(data_path, file_list[1]) %>%
  read_csv(skip=4,col_names = TRUE, guess_max = 100) %>%
  head(0)

#create new list without demographic info
new_list<- c()

for (i in file_list){
  new_list[[i]] <-
    read_csv(paste0(data_path, i),
             skip=4, col_names = TRUE, guess_max = 100) %>%
    head(-1)
}

#append all files to df
for (i in new_list){
  df<-add_row(df,i)
}
```

